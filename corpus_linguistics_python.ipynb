{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Linguistics with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session will demonstrate how Python can be used for corpus linguistics. We will work primarily with the **Natural Language Toolkit (NLTK)** library (http://www.nltk.org), which is an excellent platform for examining human language data favoured by teachers of computational linguistics. It has built-in corpora, lexical resources, and a comprehensive array of text processing libraries.\n",
    "\n",
    "After a quick review of relevant Python concepts, we will explore NLTK's core corpus linguistics functions: tokenization, frequency distributions of keywords, part-of-speech tagging, stemming, lemmatization, n-grams, collocations, and concordances. This will allow for a descriptive understanding of the corpus (word categories, counts, and contexts), which sets the stage for the detection of themes via topic modelling.\n",
    "\n",
    "We will also examine text preprocessing features offered by **scikit-learn** (http://scikit-learn.org), a user-friendly Python library for machine learning. It offers a different approach to tokenisation, and uses these tokens to transform documents into numerical vectors. Such vectors serve as input features for text classification algorithms.\n",
    "\n",
    "We will apply these functions to a few forum posts from an online newsgroups dataset, and examine them individually as well as comparatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of basic Python\n",
    "\n",
    "Let's start by reviewing some fundamental concepts, functions, and data structures in Python that will be used throughout the workshop.\n",
    "\n",
    "For a more thorough introduction to Python programming for the humanities, I recommend Folgert Karsdorp's tutorial: http://www.karsdorp.io/python-course/\n",
    "\n",
    "**Note**: I cannot underscore the importance of search engines in the programming process, and of sites such as Stack Overflow and Quora. More often than not, coding is 80% Googling for the solution and 20% adapting search results to your specific problem. Chances are that someone else has had your exact problem or something very similar to it. It's important to be familiar with basic concepts, but there is no need to memorise complex commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers, strings, and variables\n",
    "\n",
    "Numbers and strings are two of Python's built-in data types: they are the basic ingredients of the language. **Numbers** can be integers (whole numbers such as `12` and `1000`) or floats (numbers with decimal points such as `3.1415`). **Strings** are sequences of text characters and are always contained within either single `' '` or double quotes `\" \"` (but not a mixture of the two). \n",
    "\n",
    "All data types in Python are implemented as **objects** - think of them as nouns. The *type* of the object determines what actions can be done to it: numbers can, e.g., be added to each other, whereas strings have to be concatenated. Note that objects of different types cannot be concatenated or added together; they must be of the same type.\n",
    "\n",
    "We can store data types in **variables**, which require a name (label). We use the equal sign `=` to assign values to named variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hello' + 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hello ' + 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'Hello ' + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hello ' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ('Hello Python!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and methods\n",
    "\n",
    "**Functions** in Python perform actions - think of them as verbs. They are denoted by parentheses `()`. `print()` is the most basic function in Python, and is pretty self-explanatory: it prints input values directly to the screen.\n",
    "\n",
    "You can check the type of a variable with the `type()` function, and you can count the number of characters in a string with the `len()` function.\n",
    "\n",
    "**Methods** are a subcategory of functions that apply to specific objects (data types). They are called using dot notation: `object.method()`. Python has a number of useful *string methods*:\n",
    "- `.upper()` makes everything uppercase.\n",
    "- `.lower()` makes everything lowercase.\n",
    "- `.count()` adds up the number of times a character/character sequence appears.\n",
    "- `.find()` displays the position at which a character/character sequence can be found (note that Python counting starts at 0, not 1).\n",
    "- `.replace()` changes one character/character sequence for another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello Python!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.count('o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.find('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.replace('Python', 'Yin') # Put your name here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "Lists are a very useful data type in Python, and should be used to store values when order matters. They are declared using brackets `[]` and indexed starting at 0.\n",
    "\n",
    "Lists are highly *mutable*: you can add to them using the `.append()` method, remove from them using the `.remove()` method, change their values through their indexes, and slice them. For a full list of list methods (no pun intended!), see the Python documentation: https://docs.python.org/3/tutorial/datastructures.html#more-on-lists\n",
    "\n",
    "The `len()` function can be used on lists to calculate their length, and the `sorted()` function can be used to sort their elements in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list = ['London', 'New York', 'Paris']\n",
    "type(best_cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list.append('Tartu')\n",
    "best_cities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list.remove('Paris')\n",
    "best_cities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list[0] # First item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list[-1] # Last item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list[1] = 'Barcelona' # Replace the second item in a list (remember that Python counting starts at 0).\n",
    "best_cities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list[:2] # The first two items in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_list[1:] # The first item to the last item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(best_cities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Dictionaries are a powerful data type in Python that stores related information. They are declared using braces (curly brackets) `{}` and contain a list of `key:value` pairs. Unlike lists, they are not ordered, and keys are used as opposed to index numbers to extract values. Keys must be unique (a key can only have one value), but values do not have to be (different keys can have the same value). As such, they work in a similar way to the dictionaries that we know and love, apart from the fact that they aren't ordered!\n",
    "\n",
    "The `len()` function can also be used on dictionaries. Two dictionary-specific methods are `.keys()` and `.values()`, which display all of the keys and values in the dictionary, respectively.\n",
    "\n",
    "Elements can be removed from dictionaries using the `del` command.\n",
    "\n",
    "Python documentation: https://docs.python.org/3/tutorial/datastructures.html#dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_dict = {'London': 'United Kingdom', 'New York': 'United States', 'Paris': 'France'}\n",
    "type(best_cities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_dict['Barcelona'] = 'Spain' # Add to a dictionary.\n",
    "best_cities_dict # Note that the key:value pairs are not in the order in which they were entered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_dict['Paris'] # Access values of a dictionary through their keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_dict['London'] = 'England' # Change values of a dictionary through their keys.\n",
    "best_cities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_cities_dict.keys()) \n",
    "print(best_cities_dict.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_cities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_cities_dict['Paris']\n",
    "best_cities_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets\n",
    "\n",
    "A set stores an unordered collection of items for fast lookup - no indexes are used. There are no duplicates, so by definition every item in a set is unique. They are similar to dictionaries, but have no key:value pairs.\n",
    "\n",
    "Create a set with curly braces `{}` or the `set()` function. Note that if you are creating an empty set, you have to use `set()`, as `{}` represents an empty dictionary.\n",
    "\n",
    "Add to a set using the `.add` method and remove from a set using the `.remove` method. The `len()` function can be used to calculate its length. The `sorted()` function can be used to transform a set into an alphabetical list.\n",
    "\n",
    "Python documentation: https://docs.python.org/3/tutorial/datastructures.html#sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_set = {'London', 'New York', 'Paris', 'London'} \n",
    "print(type(best_cities_set))\n",
    "best_cities_set # Note that 'London' only appears once even though it is added twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_set = set()\n",
    "empty_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_set.add('Tartu')\n",
    "empty_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_set.remove('Tartu')\n",
    "empty_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(empty_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(best_cities_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If and for statements\n",
    "\n",
    "`if` and `for` statements are two of Python's most fundamental control flow tools. They are extremely intuitive and readable. `if` is used for conditional execution, whereas `for` is used to iterate over the elements of an iterable object (e.g., a list or a string).\n",
    "\n",
    "Note that the body of the statement needs to be *indented* (press tab once, or use four spaces). This is extremely important.\n",
    "\n",
    "Python documentation: https://docs.python.org/3/tutorial/controlflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(best_cities_list) >= 3: # Try changing this to 4.\n",
    "    print(best_cities_list)\n",
    "else:\n",
    "    print(\"The length of best_cities is less than 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in best_cities_dict:\n",
    "    print(city, 'is in', best_cities_dict[city])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and modules\n",
    "\n",
    "So far we have only been using the built-in functions from Python's standard library. While these are quite extensive, in order to execute more specialised tasks (e.g., those related to corpus linguistics or machine learning), we will have to import tailor-made packages and modules. A **module** is a Python file that contains functions for specific non-standard tasks; modules are organised into file hierarchies called **packages**. A **library** is a more generic term that refers to a published module, package, or group of packages.\n",
    "\n",
    "The Python Standard Library: https://docs.python.org/3/library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need these modules for most of our corpus linguistics functions.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "# We need this module for tokenization.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We need these packages for plotting.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to download a few NLTK-specific packages that were not included in the general NLTK download.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine dataset\n",
    "\n",
    "For this workshop we will be using the **20 newsgroups** dataset from scikit-learn, one of the most famous corpora for text classification and clustering. It contains 20,000 newsgroup forum posts with 20 labeled topics. To keep things simple and clear, we will examine three groups of posts from different categories: automobiles, space, and guns. This means they are highly unrelated to each other, so should contain more distinct vocabularies.\n",
    "\n",
    "When downloading the posts from our categories of interest, we will strip newsgroup-related metadata by setting the `remove` argument to equal `'headers', 'footers', 'quotes'`. We will then extract the raw text (`.data`) from each group into a separate variable. This raw text is a list: each element of the list is a forum post, so the length of the entire list represents the total number of documents (forum posts in the category).\n",
    "\n",
    "Scikit-learn documentation: http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "<br>20 newsgroups dataset homepage: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "cars = fetch_20newsgroups(categories=['rec.autos'], remove=('headers', 'footers', 'quotes'))\n",
    "space = fetch_20newsgroups(categories=['sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "guns = fetch_20newsgroups(categories=['talk.politics.guns'], remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "cars_raw = cars.data\n",
    "space_raw = space.data\n",
    "guns_raw = guns.data\n",
    "\n",
    "type(cars_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of posts about cars:', len(cars_raw))\n",
    "print('Number of posts about space:', len(space_raw))\n",
    "print('Number of posts about guns:', len(guns_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first post in each group to get a sense of their language and style. Each post is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cars_raw[0]))\n",
    "print('***')\n",
    "print(cars_raw[0])\n",
    "print('***')\n",
    "print(space_raw[0])\n",
    "print('***')\n",
    "print(guns_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now that we've downloaded the forum posts, let's tokenize them so that we can analyse their words in various ways. This will transform the strings into lists of 'words' (tokens). There are multiple ways to tokenize text:\n",
    "1. The simplest method is with Python's `.split()` method for strings. This allows us to tokenize on spaces.\n",
    "2. NLTK's `word_tokenize` is a more advanced method. It uses the Treebank Word Tokenizer and assumes that the text has already been separated into sentences. Contractions are split, and most punctuation and special characters are kept as separate tokens. \n",
    "3. Scikit-learn's `CountVectorizer` is an alternate advanced method that goes even further than NLTK. By default, words with only one character are discarded, punctuation is ignored, and special characters are stripped. \n",
    "\n",
    "Each method has its advantages and disadvantages, and there is no 'best' tokenization approach: it all depends on the research question. Scikit-learn's `CountVectorizer` disregards punctuation because such tokens are not important for most text classification algorithms, whereas NLTK retains punctuation because it is meaningful for many corpus linguistics questions. From the perspective of machine learning, selecting the right tokenization approach can be viewed as part of feature engineering: tokens are extremely significant features for text classification algorithms, as they greatly influence the output.\n",
    "\n",
    "`word_tokenize` documentation: https://www.nltk.org/api/nltk.tokenize.html\n",
    "<br> Treebank Word Tokenizer documentation: http://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "<br>`CountVectorizer` documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare tokenizers for one post\n",
    "\n",
    "Let's begin by testing the tokenizers on the first post in the space forum. As our ultimate goal is to detect topics, we can ignore capitalisation (this will reduce the number of features, which makes modelling easier). Let's also clean our text by removing new lines, which are represented by `\\n`.\n",
    "\n",
    "We can compare not only the tokens themselves, but also the total number of tokens produced by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_text = space_raw[0].lower()\n",
    "sample_text = sample_text.replace('\\n',' ')\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_python = sample_text.split(' ')\n",
    "print(type(tokenized_python))\n",
    "print(tokenized_python)\n",
    "print('Number of tokens (Python):', len(tokenized_python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_scikit = CountVectorizer().build_tokenizer()(sample_text)\n",
    "print(type(tokenized_scikit))\n",
    "print(tokenized_scikit)\n",
    "print('Number of tokens (Scikit):', len(tokenized_scikit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nltk = nltk.word_tokenize(sample_text)\n",
    "print(type(tokenized_nltk))\n",
    "print(tokenized_nltk)\n",
    "print('Number of tokens (NLTK):', len(tokenized_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As scikit-learn's `CountVectorizer` produces the 'cleanest' results for our purposes, we will use it going forward. We can modify its parameters to remove numbers, words of less than three characters, and the most common words in the English language (stopwords). These tokens are not helpful for identifying the theme of a text.\n",
    "\n",
    "We remove numbers and words of less than three characters using a *regular expression* in the `token_pattern` argument of `CountVectorizer`. `r` indicates raw string, `\\b` indicates the word boundary, `[A-Za-z]` indicates that only tokens containing alphabetic letters (either uppercase or lowercase) will be considered, and `{3,}` indicates that the token must be at least three characters. More about regular expressions: https://docs.python.org/3/library/re.html\n",
    "\n",
    "Scikit-learn stopwords: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_scikit2 = CountVectorizer(token_pattern=r'\\b[A-Za-z]{3,}\\b', stop_words='english').build_tokenizer()(sample_text)\n",
    "print(type(tokenized_scikit2))\n",
    "print(tokenized_scikit2)\n",
    "print('Number of tokens (Scikit modified):', len(tokenized_scikit2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize all forum posts\n",
    "\n",
    "Now that we have seen how different tokenizers work and selected the one best-suited to our objectives, we can scale up tokenization to the entire corpora of forum text: each corpus contains the posts in one category of interest. This will allow us to compare their vocabularies in various ways.\n",
    "\n",
    "At the moment, our corpora are represented as lists of strings (each string is a document, or forum post). To facilitate tokenization, let's merge all of the documents in each corpus into one big string using the `.join()` method. We will pass this into the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_corpus = ' '.join(cars_raw)\n",
    "cars_corpus = cars_corpus.lower()\n",
    "cars_corpus = cars_corpus.replace('\\n',' ')\n",
    "\n",
    "space_corpus = ' '.join(space_raw)\n",
    "space_corpus = space_corpus.lower()\n",
    "space_corpus = space_corpus.replace('\\n',' ')\n",
    "\n",
    "guns_corpus = ' '.join(guns_raw)\n",
    "guns_corpus = guns_corpus.lower()\n",
    "guns_corpus = guns_corpus.replace('\\n',' ')\n",
    "\n",
    "print('Number of characters in cars corpus:', len(cars_corpus))\n",
    "print('Number of characters in space corpus:', len(space_corpus))\n",
    "print('Number of characters in guns corpus:', len(guns_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 1000 characters in cars corpus:')\n",
    "print(cars_corpus[:1000])\n",
    "print('')\n",
    "print('First 1000 characters in space corpus:')\n",
    "print(space_corpus[:1000])\n",
    "print('')\n",
    "print('First 1000 characters in guns corpus:')\n",
    "print(guns_corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_cars = CountVectorizer(token_pattern=r'\\b[A-Za-z]{3,}\\b', stop_words='english').build_tokenizer()(cars_corpus)\n",
    "tokenized_space = CountVectorizer(token_pattern=r'\\b[A-Za-z]{3,}\\b', stop_words='english').build_tokenizer()(space_corpus)\n",
    "tokenized_guns = CountVectorizer(token_pattern=r'\\b[A-Za-z]{3,}\\b', stop_words='english').build_tokenizer()(guns_corpus)\n",
    "\n",
    "print('First 100 tokens in cars corpus:', tokenized_cars[:100])\n",
    "print('First 100 tokens in space corpus:', tokenized_space[:100])\n",
    "print('First 100 tokens in guns corpus:', tokenized_guns[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization allows us to count the total number of words (tokens) in each corpus, as well as the total number of *unique* words (types) in each corpus using a Python set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tokens in cars corpus:', len(tokenized_cars))\n",
    "print('Number of tokens in space corpus:', len(tokenized_space))\n",
    "print('Number of tokens in guns corpus:', len(tokenized_guns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens in cars corpus:', len(set(tokenized_cars)))\n",
    "print('Number of unique tokens in space corpus', len(set(tokenized_space)))\n",
    "print('Number of unique tokens in guns corpus:', len(set(tokenized_guns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "At the moment, words with the same root meaning such as 'car' and 'cars' are treated as separate types in our tokenized corpora. As distinguishing between these different forms is not important for topic modelling, we can simplify our tokens by **stemming** them: transforming their words into their root forms by removing affixes. This reduces the number of features, thus simplifying the model, but might not be lexicographically correct. \n",
    "\n",
    "NLTK offers three options for English words: Porter, Snowball (Porter2), and Lancaster.\n",
    "- The Porter stemming algorithm is the oldest stemming algorithm supported in NLTK, originally published in 1979. It is also the most computationally intensive. \n",
    "- The Lancaster stemming algorithm was published in 1990, and can be more aggressive than Porter; it is also the fastest. \n",
    "- The SnowballStemmer currently supports 15 languages and is nearly universally regarded as an improvement over Porter (it's in between Porter and Lancaster with regard to aggressiveness and speed).\n",
    "\n",
    "The input for these stemmers is tokenized text, which is an iterable list (not the raw text, which is a string). Let's compare their performance on our cars corpus.\n",
    "\n",
    "NLTK stemmer documentation: http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "lancaster_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_cars_porter = [porter_stemmer.stem(w) for w in tokenized_cars]\n",
    "print('First 100 tokens from Porter stemmer:', tokenized_cars_porter[:100])\n",
    "print('Number of unique tokens after Porter stemming:', len(set(tokenized_cars_porter)))\n",
    "print('')\n",
    "tokenized_cars_snowball = [snowball_stemmer.stem(w) for w in tokenized_cars]\n",
    "print('First 100 tokens from Snowball stemmer:', tokenized_cars_snowball[:100])\n",
    "print('Number of unique tokens after Snowball stemming:', len(set(tokenized_cars_snowball)))\n",
    "print('')\n",
    "tokenized_cars_lancaster = [lancaster_stemmer.stem(w) for w in tokenized_cars]\n",
    "print('First 100 tokens from Lancaster stemmer:', tokenized_cars_lancaster[:100])\n",
    "print('Number of unique tokens after Lancaster stemming:', len(set(tokenized_cars_lancaster)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech (PoS) tagging and lemmatizing\n",
    "\n",
    "As all linguists are well aware, stemming can create non-real words, such as 'thu' from 'thus'. **Lemmatization** aims to obtain the canonical (lexicographically/grammatically correct) form of the root word, the so-called *lemma*. Lemmatization is computationally more difficult and expensive than stemming, as it requires PoS tags. NLTK's lemmatizer is trained on WordNet, an English lexical database: http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html \n",
    "\n",
    "It is also useful to examine the **PoS categories** in and of themselves - they represent a level above that of specific words (types). Which words are most popular in any given PoS category? What is the frequency of different PoS categories in different corpora? NLTK's default PoS tagger uses tags from the Penn Treebank project: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_cars_pos = nltk.pos_tag(tokenized_cars)\n",
    "tokenized_space_pos = nltk.pos_tag(tokenized_space)\n",
    "tokenized_guns_pos = nltk.pos_tag(tokenized_guns)\n",
    "\n",
    "print('First ten PoS-tagged tokens in cars corpus:', tokenized_cars_pos[:10])\n",
    "print('First ten PoS-tagged tokens in space corpus:', tokenized_space_pos[:10])\n",
    "print('First ten PoS-tagged tokens in guns corpus:', tokenized_guns_pos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "    return lemmatizer.lemmatize(token, tag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_cars_lemmas = []\n",
    "\n",
    "for token, pos in tokenized_cars_pos:\n",
    "    try:\n",
    "        tokenized_cars_lemmas.append(lemmatize(token, pos))\n",
    "    except KeyError:\n",
    "        tokenized_cars_lemmas.append(token)\n",
    "\n",
    "print('First 100 tokens from lemmatization:', tokenized_cars_lemmas[:100])\n",
    "print('Number of unique tokens after lemmatization:', len(set(tokenized_cars_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_space_lemmas = []\n",
    "\n",
    "for token, pos in tokenized_space_pos:\n",
    "    try:\n",
    "        tokenized_space_lemmas.append(lemmatize(token, pos))\n",
    "    except KeyError:\n",
    "        tokenized_space_lemmas.append(token)\n",
    "\n",
    "print('First 100 tokens from lemmatization:', tokenized_space_lemmas[:100])\n",
    "print('Number of unique tokens after lemmatization:', len(set(tokenized_space_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_guns_lemmas = []\n",
    "\n",
    "for token, pos in tokenized_guns_pos:\n",
    "    try:\n",
    "        tokenized_guns_lemmas.append(lemmatize(token, pos))\n",
    "    except KeyError:\n",
    "        tokenized_guns_lemmas.append(token)\n",
    "\n",
    "print('First 100 tokens from lemmatization:', tokenized_guns_lemmas[:100])\n",
    "print('Number of unique tokens after lemmatization:', len(set(tokenized_guns_lemmas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rank the frequencies of the different PoS tags in each corpus, and plot the frequencies to visually compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_tag_fd = nltk.FreqDist(tag for (word, tag) in tokenized_cars_pos)\n",
    "space_tag_fd = nltk.FreqDist(tag for (word, tag) in tokenized_space_pos)\n",
    "guns_tag_fd = nltk.FreqDist(tag for (word, tag) in tokenized_guns_pos)\n",
    "\n",
    "print('Most common PoS tags in cars corpus:', cars_tag_fd.most_common())\n",
    "print('')\n",
    "print('Most common PoS tags in space corpus:', space_tag_fd.most_common())\n",
    "print('')\n",
    "print('Most common PoS tags in guns corpus:', guns_tag_fd.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('PoS Tag Frequencies in Cars Corpus')\n",
    "cars_tag_fd.plot()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('PoS Tag Frequencies in Space Corpus')\n",
    "space_tag_fd.plot()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('PoS Tag Frequencies in Guns Corpus')\n",
    "guns_tag_fd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PoS tag, we can filter for only one lexical category (e.g., nouns). To compare its frequency across corpora, we can calculate the proportion of words with the tag in the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_corpus_nouns = [(token, pos) for token, pos in tokenized_cars_pos if pos.startswith('N')]\n",
    "space_corpus_nouns = [(token, pos) for token, pos in tokenized_space_pos if pos.startswith('N')]\n",
    "guns_corpus_nouns = [(token, pos) for token, pos in tokenized_guns_pos if pos.startswith('N')]\n",
    "\n",
    "print('Proportion of nouns in cars corpus:', len(cars_corpus_nouns)/len(tokenized_cars))\n",
    "print('First ten nouns in cars corpus:', cars_corpus_nouns[:10])\n",
    "print('Proportion of nouns in space corpus:', len(space_corpus_nouns)/len(tokenized_space))\n",
    "print('First ten nouns in space corpus:', space_corpus_nouns[:10])\n",
    "print('Proportion of nouns in guns corpus:', len(guns_corpus_nouns)/len(tokenized_guns))\n",
    "print('First ten nouns in guns corpus:', guns_corpus_nouns[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word-level calculations\n",
    "\n",
    "Tokenization allows us to count the words (tokens) in a text in a variety of different ways. We have already counted the total number of tokens and types (unique tokens) in each corpus. From these counts, we can write our own function to calculate *lexical richness*: the number of types (unique words) divided by the number of tokens (total words). We can use the lemmatized corpora to group words together that have the same root meaning, as they do not increase semantic diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(corpus):\n",
    "    return len(set(corpus))/len(corpus)\n",
    "\n",
    "print('Lexical diversity of cars corpus:', lexical_diversity(tokenized_cars_lemmas))\n",
    "print('Lexical diversity of space corpus:', lexical_diversity(tokenized_space_lemmas))\n",
    "print('Lexical diversity of guns corpus:', lexical_diversity(tokenized_guns_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the frequency of specific words (types) in the forum posts. After we generate the frequency distribution, we can see what the most common words are in each corpus. We can also display the PoS categories of the most common words, and sort the words within a certain PoS category by frequency: e.g., what are the most popular *nouns* in each corpus? Again, we will use the lemmatized corpora to group words together that have the same root meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_cars = FreqDist(tokenized_cars_lemmas)\n",
    "fdist_space = FreqDist(tokenized_space_lemmas)\n",
    "fdist_guns = FreqDist(tokenized_guns_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist_cars['car'])\n",
    "print(fdist_space['space'])\n",
    "print(fdist_guns['gun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent words in cars corpus:', fdist_cars.most_common(20))\n",
    "print('Most frequent words in space corpus:', fdist_space.most_common(20))\n",
    "print('Most frequent words in guns corpus:', fdist_guns.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_type_tag_fd = nltk.FreqDist(tokenized_cars_pos)\n",
    "space_type_tag_fd = nltk.FreqDist(tokenized_space_pos)\n",
    "guns_type_tag_fd = nltk.FreqDist(tokenized_guns_pos)\n",
    "\n",
    "print('Most frequent words and their PoS in cars corpus:', cars_type_tag_fd.most_common(20))\n",
    "print('Most frequent words and their PoS in space corpus:', space_type_tag_fd.most_common(20))\n",
    "print('Most frequent words and their PoS in guns corpus:', guns_type_tag_fd.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most popular nouns in cars corpus:', [typetag[0] for (typetag, _) in cars_type_tag_fd.most_common() if typetag[1] == 'NN'][:10])\n",
    "print('Most popular nouns in space corpus:', [typetag[0] for (typetag, _) in space_type_tag_fd.most_common() if typetag[1] == 'NN'][:10])\n",
    "print('Most popular nouns in guns corpus:', [typetag[0] for (typetag, _) in guns_type_tag_fd.most_common() if typetag[1] == 'NN'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some more peculiar words: those that appear only once (hapax legomena), those that are extremely long, and those that are both long and frequently occurring. Such words often add a different perspective on a corpus of text (they're a bit like linguistic outliers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of hapax legomena in cars corpus:', len(fdist_cars.hapaxes()))\n",
    "print('Number of hapax legomena in space corpus:', len(fdist_space.hapaxes()))\n",
    "print('Number of hapax legomena in guns corpus:', len(fdist_guns.hapaxes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 20 hapax legomena in cars corpus:', fdist_cars.hapaxes()[:20])\n",
    "print('First 20 hapax legomena in space corpus:', fdist_space.hapaxes()[:20])\n",
    "print('First 20 hapax legomena in guns corpus:', fdist_guns.hapaxes()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cars = set(tokenized_cars_lemmas)\n",
    "vocab_space = set(tokenized_space_lemmas)\n",
    "vocab_guns = set(tokenized_guns_lemmas)\n",
    "\n",
    "# Let's define a long word as a word with more than 10 characters.\n",
    "long_words_cars = [word for word in vocab_cars if len(word) > 10] \n",
    "long_words_space = [word for word in vocab_space if len(word) > 10]\n",
    "long_words_guns = [word for word in vocab_guns if len(word) > 10]\n",
    "\n",
    "print('First 20 long words in cars corpus:', long_words_cars[:20])\n",
    "print('First 20 long words in space corpus:', long_words_space[:20])\n",
    "print('First 20 long words in guns corpus:', long_words_guns[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define frequent words as those that occur more than 10 times.\n",
    "long_frequent_words_cars = [word for word in vocab_cars if len(word) > 10 and fdist_cars[word] > 10]\n",
    "long_frequent_words_space = [word for word in vocab_space if len(word) > 10 and fdist_space[word] > 10]\n",
    "long_frequent_words_guns = [word for word in vocab_guns if len(word) > 10 and fdist_guns[word] > 10]\n",
    "\n",
    "print('Long frequent words in cars corpus:', sorted(long_frequent_words_cars))\n",
    "print('Long frequent words in space corpus:', sorted(long_frequent_words_space))\n",
    "print('Long frequent words in guns corpus:', sorted(long_frequent_words_guns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams and collocations\n",
    "\n",
    "N-grams are words that co-occur within a given window: 2-grams (bigrams) are two words that co-occur, 3-grams (trigrams) are three words that co-occur, etc. The window is typically just one word (i.e., the words must be next to each other). \n",
    "\n",
    "N-gram collocations are n-grams that occur more often than we would expect based on the frequency of the individual words. We can compute them in Python using Pointwise Mutual Information (a measure of association used in statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder_cars = BigramCollocationFinder.from_words(tokenized_cars)\n",
    "bigram_finder_space = BigramCollocationFinder.from_words(tokenized_space)\n",
    "bigram_finder_guns = BigramCollocationFinder.from_words(tokenized_guns)\n",
    "\n",
    "# Top 20 bigrams.\n",
    "print('Top 20 bigrams in cars corpus:', bigram_finder_cars.nbest(bigram_measures.pmi, 20))\n",
    "print('Top 20 bigrams in space corpus:', bigram_finder_space.nbest(bigram_measures.pmi, 20))\n",
    "print('Top 20 bigrams in guns corpus:', bigram_finder_guns.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the results to only see the top 20 bigrams that appear at least five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigram_finder_cars.apply_freq_filter(5)\n",
    "bigram_finder_space.apply_freq_filter(5)\n",
    "bigram_finder_guns.apply_freq_filter(5)\n",
    "\n",
    "print('Top 20 frequent bigrams in cars corpus:', bigram_finder_cars.nbest(bigram_measures.pmi, 20))\n",
    "print('Top 20 frequent bigrams in space corpus:', bigram_finder_space.nbest(bigram_measures.pmi, 20))\n",
    "print('Top 20 frequent bigrams in guns corpus:', bigram_finder_guns.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply a word filter to remove bigrams containing specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_finder_cars.apply_freq_filter(5)\n",
    "bigram_finder_space.apply_freq_filter(5)\n",
    "bigram_finder_guns.apply_freq_filter(5)\n",
    "\n",
    "bigram_finder_cars.apply_word_filter(lambda w: w in ('gov', 'blah'))\n",
    "bigram_finder_space.apply_word_filter(lambda w: w in ('emx'))\n",
    "bigram_finder_guns.apply_word_filter(lambda w: w in ('ifas'))\n",
    "\n",
    "print('Top 20 frequent bigrams in cars corpus (filtered):', bigram_finder_cars.nbest(bigram_measures.pmi, 20))\n",
    "print('Top 20 frequent bigrams in space corpus (filtered):', bigram_finder_space.nbest(bigram_measures.pmi, 20))\n",
    "print('Top 20 frequent bigrams in guns corpus (filtered):', bigram_finder_guns.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for trigrams.\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigram_finder_cars = TrigramCollocationFinder.from_words(tokenized_cars)\n",
    "trigram_finder_space = TrigramCollocationFinder.from_words(tokenized_space)\n",
    "trigram_finder_guns = TrigramCollocationFinder.from_words(tokenized_guns)\n",
    "\n",
    "trigram_finder_cars.apply_freq_filter(5)\n",
    "trigram_finder_space.apply_freq_filter(5)\n",
    "trigram_finder_guns.apply_freq_filter(5)\n",
    "\n",
    "print('Top 20 frequent trigrams in cars corpus:', trigram_finder_cars.nbest(trigram_measures.pmi, 20))\n",
    "print('Top 20 frequent trigrams in space corpus:', trigram_finder_space.nbest(trigram_measures.pmi, 20))\n",
    "print('Top 20 frequent trigrams in guns corpus:', trigram_finder_guns.nbest(trigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching text (in context)\n",
    "\n",
    "We've been looking at words in isolation as well as in clusters of two and three (bigrams and trigrams). Let's zoom out and examine the context of these words. NLTK provides some very useful functions for this.\n",
    "\n",
    "To preserve as much of the original context as we can (including punctuation and special characters), we can tokenize the corpora with NLTK's `word_tokenize`. These tokenized corpora then have to be transformed into NLTK Text objects in order for the NLTK-specific methods to be executed on them:\n",
    "- **Concordances** allow us to see keywords in context (KWIC).\n",
    "- For any given word, we can calculate **similar words** in the corpus: which words occur in a similar range of contexts?\n",
    "- For any given set of two or more words, we can examine the **common contexts** that they share in the corpus (if any).\n",
    "- We can visualise the locations of words in the text by generating a **lexical dispersion plot**. Each stripe in the plot represents an instance of a word, and each row represents the entire text. Location is measured by 'word offset': the number of characters from the beginning of the text at which the word can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_cars_nltk = nltk.word_tokenize(cars_corpus)\n",
    "tokenized_space_nltk = nltk.word_tokenize(space_corpus)\n",
    "tokenized_guns_nltk = nltk.word_tokenize(guns_corpus)\n",
    "\n",
    "cars_text = nltk.Text(tokenized_cars_nltk)\n",
    "space_text = nltk.Text(tokenized_space_nltk)\n",
    "guns_text = nltk.Text(tokenized_guns_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_text.concordance('acceleration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_text.concordance('acceleration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_text.similar('space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_text.common_contexts(['space', 'lunar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "plt.figure(figsize=(15, 5)) \n",
    "space_text.dispersion_plot(['sun', 'earth', 'moon', 'satellite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Ended Exercises and Questions\n",
    "1. Rerun the above analyses for other forum topics. It would be interesting to compare two topics in the same category: e.g., comp.sys.ibm.pc.hardware and comp.sys.mac.hardware (PC vs Mac hardware).\n",
    "2. What are the most common adjectives in the forum corpora? The most common verbs?\n",
    "3. What are the most common short words (length < 5)?\n",
    "4. Play around with the various tools that NLTK provides for searching text (concordances, similar words, lexical dispersion plots) with different corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
